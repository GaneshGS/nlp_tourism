{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Requirement already satisfied: keras==2.2.4 in /home/chantana/.local/lib/python2.7/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/chantana/.local/lib/python2.7/site-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib64/python2.7/site-packages (from keras==2.2.4) (3.13)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/chantana/.local/lib/python2.7/site-packages (from keras==2.2.4) (1.14.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python2.7/site-packages (from keras==2.2.4) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/chantana/.local/lib/python2.7/site-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: h5py in /home/chantana/.local/lib/python2.7/site-packages (from keras==2.2.4) (2.7.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/lib64/python2.7/site-packages (from keras==2.2.4) (1.2.0)\n",
      "\u001b[33mYou are using pip version 19.0.2, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chantana/.local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras  as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim import logging\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as  pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from  keras.preprocessing.text import Tokenizer\n",
    "from  keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/chantana/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delicious', 'ambrosial', 'palatable', 'yummy', 'yumy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_files = ['foodreview.txt']\n",
    "word_dishes = []\n",
    "for word_file in word_files:\n",
    "    with open(word_file) as f:\n",
    "        lines = f.readlines()\n",
    "    word_list2 = [x.strip() for x in lines] \n",
    "    \n",
    "    word_dishes.extend(word_list2)\n",
    "word_dishes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food service', 'service', 'best restaurant', 'enjoy', 'chill']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_files = ['restaurantreview.txt']\n",
    "word_restaurant = []\n",
    "for word_file in word_files:\n",
    "    with open(word_file) as f:\n",
    "        lines = f.readlines()\n",
    "    word_list2 = [x.strip() for x in lines] \n",
    "    \n",
    "    word_restaurant.extend(word_list2)\n",
    "word_restaurant[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nice place', 'place', 'location', 'great place', 'located']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_files = ['location.txt']\n",
    "word_location = []\n",
    "for word_file in word_files:\n",
    "    with open(word_file) as f:\n",
    "        lines = f.readlines()\n",
    "    lists_l= [x.strip() for x in lines] \n",
    "    \n",
    "    word_location.extend(lists_l)\n",
    "word_location[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "def ordered_set(in_list):\n",
    "    out_list = []\n",
    "    added = set()\n",
    "    for val in in_list:\n",
    "        if not val in added:\n",
    "            out_list.append(val)\n",
    "            added.add(val)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dishes = ordered_set(word_dishes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_restaurant = ordered_set(word_restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_location = ordered_set(word_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aci', 'bowls'),\n",
       " ('baba', 'Ghanoush'),\n",
       " ('baby', 'Back', 'Ribs'),\n",
       " ('bak', 'kut', 'teh'),\n",
       " ('bananas', 'foster')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mw_list = []\n",
    "for x in word_dishes:\n",
    "    ws = x.split() \n",
    "    if len(ws) > 1:\n",
    "        tws = tuple(ws)\n",
    "        mw_list.append(tws)\n",
    "        \n",
    "\n",
    "for x in word_restaurant:\n",
    "    ws = x.split() \n",
    "    if len(ws) > 1:\n",
    "        tws = tuple(ws)\n",
    "        mw_list.append(tws)\n",
    "        \n",
    "\n",
    "        \n",
    "for x in word_location:\n",
    "    ws = x.split() \n",
    "    if len(ws) > 1:\n",
    "        tws = tuple(ws)\n",
    "        mw_list.append(tws)\n",
    "        \n",
    "mw_list = ordered_set(mw_list)\n",
    "mw_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n"
     ]
    }
   ],
   "source": [
    "my_idx = {}\n",
    "for i,w in enumerate(mw_list):\n",
    "    my_idx[\" \".join(w)] = i\n",
    "print(len(my_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n",
      "found\n"
     ]
    }
   ],
   "source": [
    "for i in mw_list:\n",
    "    if i == ('friendly','service'):\n",
    "        print('found')\n",
    "    if i == ('asian','food'):\n",
    "        print('found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe = MWETokenizer(mw_list,separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"review_restaurant.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.dropna()\n",
    "reviews = reviews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans(txt):\n",
    "    tokens=mwe.tokenize(word_tokenize(txt.lower()))\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        yield token, offset, offset+len(token)\n",
    "        offset += len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        # We are not using \"text.split()\" here\n",
    "        #since it is not fool proof, e.g. words followed by punctuations \"Are you kidding?I think you aren't.\"\n",
    "        text = re.findall(r\"[\\w']+\", text)\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)# remove links\n",
    "    text = re.sub(r'\\<a href', ' ', text)# remove html link tag\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.!?:#$\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurants are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_restaurant = []\n",
    "for restaurants in reviews.restaurants:\n",
    "    clean_restaurant.append(clean_text(restaurants, remove_stopwords=False))\n",
    "print(\"restaurants are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb49619dbaa4365ae85dd7a988bc35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18684), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "all_item = []\n",
    "\n",
    "for i in tqdm(range(len(reviews))):\n",
    "    word_ls = []\n",
    "    #print(\"sentence:\",i+1)\n",
    "    \n",
    "    for token in spans(clean_restaurant[i]):\n",
    "        #print(token)\n",
    "        #assert token[0]==reviews.location[i][token[1]:token[2]]\n",
    "        my_tuple = token[0]\n",
    "        #print(\"###\",token)\n",
    "        \n",
    "        #my_tuples = ' , '.join(map(str, my_tuple))\n",
    "        if token[0] in word_dishes:\n",
    "            #word_ls.append(my_tuple)\n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-FOOD']*len(pos_list)\n",
    "            tag_list[0] = 'B-FOOD' \n",
    "            \n",
    "            \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):           \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                    #print('list')\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)\n",
    "                \n",
    "                \n",
    "            #lis_lo = nltk.pos_tag(word_ls),LOC\n",
    "            #print(' , '.join(map(str, lis_lo)))\n",
    " \n",
    "        elif token[0] in word_restaurant:\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-RORG']*len(pos_list)\n",
    "            tag_list[0] = 'B-RORG' \n",
    "            \n",
    "            #print(subwords)\n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):           \n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p[0][1]})\n",
    "                all_item.append(new_item)\n",
    "                #print(new_item)\n",
    "            #word_ls.append(my_tuple)\n",
    "            #print(\"found LOC\")\n",
    "            #lis_lo = nltk.pos_tag(word_ls),\"LOC\"\n",
    "            #print(' , '.join(map(str, lis_lo)))\n",
    "             \n",
    "        elif token[0] in word_location:\n",
    "            #word_ls.append(my_tuple)\n",
    "            #lis_lo = nltk.pos_tag(word_ls),\"FACILITY\"\n",
    "            #print(' , '.join(map(str, lis_lo)))\n",
    "            \n",
    "            #my_pos = nltk.pos_tag([my_tuple])[0][1]\n",
    "            #new_item =  dict({'Sentence #': i+1, 'Tag' : 'FACILITY', 'Word': my_tuple,'POS': my_pos})\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-RLOC']*len(pos_list)\n",
    "            tag_list[0] = 'B-RLOC' \n",
    "            \n",
    "             \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):   \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)\n",
    "                \n",
    "\n",
    "             \n",
    "        else:\n",
    "            #print(type(my_tuple))\n",
    "            my_pos = nltk.pos_tag([my_tuple])[0][1]\n",
    "            new_item = dict({'Sentence #': i+1, 'Tag' : 'O', 'Word': my_tuple.lower(),'POS': my_pos})\n",
    "            all_item.append(new_item)\n",
    "            \n",
    "            #if i+1 == 5177:\n",
    "                #print(new_item)\n",
    "             \n",
    "            #print(nltk.pos_tag([my_tuple]),',','O')\n",
    "        #print(new_item)\n",
    "        #if not(my_pos == '.' or  my_pos == ',' or my_pos == ':' or my_pos == '(' or my_pos == ')') :\n",
    "            \n",
    "            #all_item.append(new_item) \n",
    "file_csv = 'restaurant-bert.csv'\n",
    "with open(file_csv, 'w') as csv_file:\n",
    "    csv_file.write('Sentence #*Word*POS*Tag\\n')\n",
    "    for item in all_item:      \n",
    "        #print(item['POS'])    \n",
    "        csv_file.write(str(item['Sentence #'])+'*'+item['Word']+'*'+item['POS']+'*'+item['Tag']+'\\n')\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "#load data\n",
    "# explore data\n",
    "file_csv = 'restaurant-bert.csv'\n",
    "data = pd.read_csv(file_csv, sep='*',encoding=\"utf-8\").fillna(method=\"ffill\")\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [ [s[0] for s in sent] for sent in getter.sentences] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word_list\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "print(labels[0])\n",
    "print(len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = list(set(data[\"Tag\"].values))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i: t for i, t in enumerate(tags_vals) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21127"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words); \n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I-FOOD'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('pytorch-pretrained-BERT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "bs = 1286\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "#torch.cuda.get_device_name(0)\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'food', 'and', 'friendly', 'service', 'the', 'restaurant', 'is', 'on', 'upper', 'night', 'market', 'close', 'to', 'the', 'railway', 'road', 'fish', 'and', 'seafood', 'is', 'very', 'popular', 'here', 'not', 'expensive', 'and', 'always', 'fresh', 'fish', 'and', 'summarization', 'on', '135', 'is', 'nice', 'place', 'in', 'the', 'end', 'of', 'the', 'night', 'market']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = word_list\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good',\n",
       " 'food',\n",
       " 'and',\n",
       " 'friendly',\n",
       " 'service',\n",
       " 'the',\n",
       " 'restaurant',\n",
       " 'is',\n",
       " 'on',\n",
       " 'upper',\n",
       " 'night',\n",
       " 'market',\n",
       " 'close',\n",
       " 'to',\n",
       " 'the',\n",
       " 'railway',\n",
       " 'road',\n",
       " 'fish',\n",
       " 'and',\n",
       " 'seafood',\n",
       " 'is',\n",
       " 'very',\n",
       " 'popular',\n",
       " 'here',\n",
       " 'not',\n",
       " 'expensive',\n",
       " 'and',\n",
       " 'always',\n",
       " 'fresh',\n",
       " 'fish',\n",
       " 'and',\n",
       " 'summarization',\n",
       " 'on',\n",
       " '135',\n",
       " 'is',\n",
       " 'nice',\n",
       " 'place',\n",
       " 'in',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'night',\n",
       " 'market']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44\n"
     ]
    }
   ],
   "source": [
    "tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n",
    "print(len(tokens_ids[0]),len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_ids[0]),len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "input_ids = pad_sequences(tokens_ids,\n",
    "                          maxlen=MAX_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")\n",
    "#input_ids = pad_sequences([convert_word2idx(txt) for txt in tokenized_texts],\n",
    "                          #maxlen=MAX_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "need more\n",
      "need more\n",
      "need more\n",
      "need more\n",
      "need more\n",
      "need more\n",
      "need more\n",
      "need more\n",
      "need more\n",
      "need more\n",
      "need more\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(MAX_LEN)\n",
    "for i in tokens_ids:\n",
    "    if len(i) > MAX_LEN:\n",
    "         \n",
    "        i = i[:128]\n",
    "        print(\"need more\")\n",
    "         \n",
    "        #MAX_LEN = len(i)\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n",
    "len(t_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1989  6588 21120 20059 18093 19834  4319 20502 18538  6250 19136  4767\n",
      " 17986 12982 19834 15575  8943  4943 21120  1403 20502 14737 14561 10435\n",
      "  1529  7069 21120  7986  8881  4943 21120 20728 18538  7312 20502 15372\n",
      "  5688  3679 19834  4299 11860 19834 19136  4767     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"int64\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train test\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to torhc tensor\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16815, 128])\n",
      "torch.Size([16815, 128])\n",
      "torch.Size([16815, 128])\n"
     ]
    }
   ],
   "source": [
    "print(tr_inputs.shape)\n",
    "print(tr_masks.shape)\n",
    "print(tr_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1869, 128])\n",
      "torch.Size([1869, 128])\n",
      "torch.Size([1869, 128])\n"
     ]
    }
   ],
   "source": [
    "print(val_inputs.shape)\n",
    "print(val_masks.shape)\n",
    "print(val_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(u\"bert-large-uncased\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune BERT\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [07:45<1:09:50, 465.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9344372574385511\n",
      "Validation loss: 0.042146968749998986\n",
      "Validation Accuracy: 0.9945550592812907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 2/10 [15:30<1:02:03, 465.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9765140863406758\n",
      "Validation loss: 0.014456866549769952\n",
      "Validation Accuracy: 0.9980185457749348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 3/10 [23:15<54:16, 465.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9826914080773428\n",
      "Validation loss: 0.00940897184748684\n",
      "Validation Accuracy: 0.9984151549258474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 4/10 [31:00<46:30, 465.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9729173102667078\n",
      "Validation loss: 0.008257880343234766\n",
      "Validation Accuracy: 0.9976862344157431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 5/10 [38:44<38:44, 464.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9808950264770013\n",
      "Validation loss: 0.00662525952251044\n",
      "Validation Accuracy: 0.9983677273468058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 6/10 [46:28<30:58, 464.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9892743159106366\n",
      "Validation loss: 0.0063096710005469256\n",
      "Validation Accuracy: 0.9991268869173728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 7/10 [54:12<23:13, 464.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9875332533514162\n",
      "Validation loss: 0.005990897633591115\n",
      "Validation Accuracy: 0.9989530918961864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 8/10 [1:01:56<15:28, 464.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9912097111762245\n",
      "Validation loss: 0.004572338753952586\n",
      "Validation Accuracy: 0.9993255097987288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 9/10 [1:09:40<07:44, 464.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9900182910896264\n",
      "Validation loss: 0.00564116221783294\n",
      "Validation Accuracy: 0.9991765426377118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 100%|██████████| 10/10 [1:17:24<00:00, 464.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.983991683991684\n",
      "Validation loss: 0.0052122775518524825\n",
      "Validation Accuracy: 0.9986573857154498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    #print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels, true_inputs = [], [],[]\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        inputs = b_input_ids.to('cpu').numpy()\n",
    "         \n",
    "        true_inputs.append(inputs)\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n",
    "\n",
    "\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
