{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chantana/.local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras  as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim import logging\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as  pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from  keras.preprocessing.text import Tokenizer\n",
    "from  keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/chantana/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delicious', 'ambrosial', 'palatable', 'yummy', 'yumy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_files = ['foodreview.txt']\n",
    "word_dishes = []\n",
    "for word_file in word_files:\n",
    "    with open(word_file) as f:\n",
    "        lines = f.readlines()\n",
    "    word_list2 = [x.strip() for x in lines] \n",
    "    \n",
    "    word_dishes.extend(word_list2)\n",
    "word_dishes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food service', 'service', 'best restaurant', 'enjoy', 'chill']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_files = ['restaurantreview.txt']\n",
    "word_restaurant = []\n",
    "for word_file in word_files:\n",
    "    with open(word_file) as f:\n",
    "        lines = f.readlines()\n",
    "    word_list2 = [x.strip() for x in lines] \n",
    "    \n",
    "    word_restaurant.extend(word_list2)\n",
    "word_restaurant[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nice place', 'place', 'location', 'great place', 'located']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_files = ['location.txt']\n",
    "word_location = []\n",
    "for word_file in word_files:\n",
    "    with open(word_file) as f:\n",
    "        lines = f.readlines()\n",
    "    lists_l= [x.strip() for x in lines] \n",
    "    \n",
    "    word_location.extend(lists_l)\n",
    "word_location[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "def ordered_set(in_list):\n",
    "    out_list = []\n",
    "    added = set()\n",
    "    for val in in_list:\n",
    "        if not val in added:\n",
    "            out_list.append(val)\n",
    "            added.add(val)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dishes = ordered_set(word_dishes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_restaurant = ordered_set(word_restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_location = ordered_set(word_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aci', 'bowls'),\n",
       " ('baba', 'Ghanoush'),\n",
       " ('baby', 'Back', 'Ribs'),\n",
       " ('bak', 'kut', 'teh'),\n",
       " ('bananas', 'foster')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mw_list = []\n",
    "for x in word_dishes:\n",
    "    ws = x.split() \n",
    "    if len(ws) > 1:\n",
    "        tws = tuple(ws)\n",
    "        mw_list.append(tws)\n",
    "        \n",
    "\n",
    "for x in word_restaurant:\n",
    "    ws = x.split() \n",
    "    if len(ws) > 1:\n",
    "        tws = tuple(ws)\n",
    "        mw_list.append(tws)\n",
    "        \n",
    "\n",
    "        \n",
    "for x in word_location:\n",
    "    ws = x.split() \n",
    "    if len(ws) > 1:\n",
    "        tws = tuple(ws)\n",
    "        mw_list.append(tws)\n",
    "        \n",
    "mw_list = ordered_set(mw_list)\n",
    "mw_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n"
     ]
    }
   ],
   "source": [
    "my_idx = {}\n",
    "for i,w in enumerate(mw_list):\n",
    "    my_idx[\" \".join(w)] = i\n",
    "print(len(my_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n",
      "found\n"
     ]
    }
   ],
   "source": [
    "for i in mw_list:\n",
    "    if i == ('friendly','service'):\n",
    "        print('found')\n",
    "    if i == ('asian','food'):\n",
    "        print('found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe = MWETokenizer(mw_list,separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"review_restaurant.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.dropna()\n",
    "reviews = reviews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans(txt):\n",
    "    tokens=mwe.tokenize(word_tokenize(txt.lower()))\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        yield token, offset, offset+len(token)\n",
    "        offset += len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        # We are not using \"text.split()\" here\n",
    "        #since it is not fool proof, e.g. words followed by punctuations \"Are you kidding?I think you aren't.\"\n",
    "        text = re.findall(r\"[\\w']+\", text)\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)# remove links\n",
    "    text = re.sub(r'\\<a href', ' ', text)# remove html link tag\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.!?:#$\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurants are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_restaurant = []\n",
    "for restaurants in reviews.restaurants:\n",
    "    clean_restaurant.append(clean_text(restaurants, remove_stopwords=False))\n",
    "print(\"restaurants are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009d8ecb5a3d4791a5350c5bd444e413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18684), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "all_item = []\n",
    "\n",
    "for i in tqdm(range(len(reviews))):\n",
    "    word_ls = []\n",
    "    #print(\"sentence:\",i+1)\n",
    "    \n",
    "    for token in spans(clean_restaurant[i]):\n",
    "        #print(token)\n",
    "        #assert token[0]==reviews.location[i][token[1]:token[2]]\n",
    "        my_tuple = token[0]\n",
    "        #print(\"###\",token)\n",
    "        \n",
    "        #my_tuples = ' , '.join(map(str, my_tuple))\n",
    "        if token[0] in word_dishes:\n",
    "            #word_ls.append(my_tuple)\n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-FOOD']*len(pos_list)\n",
    "            tag_list[0] = 'B-FOOD' \n",
    "            \n",
    "            \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):           \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                    #print('list')\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)\n",
    "                \n",
    "                \n",
    "            #lis_lo = nltk.pos_tag(word_ls),LOC\n",
    "            #print(' , '.join(map(str, lis_lo)))\n",
    " \n",
    "        elif token[0] in word_restaurant:\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-RORG']*len(pos_list)\n",
    "            tag_list[0] = 'B-RORG' \n",
    "            \n",
    "            #print(subwords)\n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):           \n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p[0][1]})\n",
    "                all_item.append(new_item)\n",
    "                #print(new_item)\n",
    "            #word_ls.append(my_tuple)\n",
    "            #print(\"found LOC\")\n",
    "            #lis_lo = nltk.pos_tag(word_ls),\"LOC\"\n",
    "            #print(' , '.join(map(str, lis_lo)))\n",
    "             \n",
    "        elif token[0] in word_location:\n",
    "            #word_ls.append(my_tuple)\n",
    "            #lis_lo = nltk.pos_tag(word_ls),\"FACILITY\"\n",
    "            #print(' , '.join(map(str, lis_lo)))\n",
    "            \n",
    "            #my_pos = nltk.pos_tag([my_tuple])[0][1]\n",
    "            #new_item =  dict({'Sentence #': i+1, 'Tag' : 'FACILITY', 'Word': my_tuple,'POS': my_pos})\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-RLOC']*len(pos_list)\n",
    "            tag_list[0] = 'B-RLOC' \n",
    "            \n",
    "             \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):   \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)\n",
    "                \n",
    "\n",
    "             \n",
    "        else:\n",
    "            #print(type(my_tuple))\n",
    "            my_pos = nltk.pos_tag([my_tuple])[0][1]\n",
    "            new_item = dict({'Sentence #': i+1, 'Tag' : 'O', 'Word': my_tuple.lower(),'POS': my_pos})\n",
    "            all_item.append(new_item)\n",
    "            \n",
    "            #if i+1 == 5177:\n",
    "                #print(new_item)\n",
    "             \n",
    "            #print(nltk.pos_tag([my_tuple]),',','O')\n",
    "        #print(new_item)\n",
    "        #if not(my_pos == '.' or  my_pos == ',' or my_pos == ':' or my_pos == '(' or my_pos == ')') :\n",
    "            \n",
    "            #all_item.append(new_item) \n",
    "file_csv = 'restaurant-bert.csv'\n",
    "with open(file_csv, 'w') as csv_file:\n",
    "    csv_file.write('Sentence #*Word*POS*Tag\\n')\n",
    "    for item in all_item:      \n",
    "        #print(item['POS'])    \n",
    "        csv_file.write(str(item['Sentence #'])+'*'+item['Word']+'*'+item['POS']+'*'+item['Tag']+'\\n')\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-FOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>food</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-FOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>friendly</td>\n",
       "      <td>RB</td>\n",
       "      <td>B-RORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>service</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-RORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>upper</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #        Word  POS     Tag\n",
       "0           1        good   JJ  B-FOOD\n",
       "1           1        food   NN  I-FOOD\n",
       "2           1         and   CC       O\n",
       "3           1    friendly   RB  B-RORG\n",
       "4           1     service   NN  I-RORG\n",
       "5           1         the   DT       O\n",
       "6           1  restaurant   NN       O\n",
       "7           1          is  VBZ       O\n",
       "8           1          on   IN       O\n",
       "9           1       upper   JJ       O"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "#load data\n",
    "# explore data\n",
    "file_csv = 'restaurant-bert.csv'\n",
    "data = pd.read_csv(file_csv, sep='*',encoding=\"utf-8\").fillna(method=\"ffill\")\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [ [s[0] for s in sent] for sent in getter.sentences] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'good',\n",
       " u'food',\n",
       " u'and',\n",
       " u'friendly',\n",
       " u'service',\n",
       " u'the',\n",
       " u'restaurant',\n",
       " u'is',\n",
       " u'on',\n",
       " u'upper',\n",
       " u'night',\n",
       " u'market',\n",
       " u'close',\n",
       " u'to',\n",
       " u'the',\n",
       " u'railway',\n",
       " u'road',\n",
       " u'fish',\n",
       " u'and',\n",
       " u'seafood',\n",
       " u'is',\n",
       " u'very',\n",
       " u'popular',\n",
       " u'here',\n",
       " u'not',\n",
       " u'expensive',\n",
       " u'and',\n",
       " u'always',\n",
       " u'fresh',\n",
       " u'fish',\n",
       " u'and',\n",
       " u'summarization',\n",
       " u'on',\n",
       " u'135',\n",
       " u'is',\n",
       " u'nice',\n",
       " u'place',\n",
       " u'in',\n",
       " u'the',\n",
       " u'end',\n",
       " u'of',\n",
       " u'the',\n",
       " u'night',\n",
       " u'market']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = word_list\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18684\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'B-FOOD', u'I-FOOD', u'O', u'B-RORG', u'I-RORG', u'O', u'O', u'O', u'O', u'O', u'O', u'B-RLOC', u'B-RLOC', u'I-RLOC', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'B-RLOC', u'I-RLOC', u'O', u'O', u'O', u'O', u'O', u'O', u'B-RLOC']\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "print(labels[0])\n",
    "print(len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18684\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = list(set(data[\"Tag\"].values))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i: t for i, t in enumerate(tags_vals) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19476"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words); \n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'B-FOOD'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('pytorch-pretrained-BERT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "bs = 8\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "#torch.cuda.get_device_name(0)\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'good', u'food', u'and', u'friendly', u'service', u'the', u'restaurant', u'is', u'on', u'upper', u'night', u'market', u'close', u'to', u'the', u'railway', u'road', u'fish', u'and', u'seafood', u'is', u'very', u'popular', u'here', u'not', u'expensive', u'and', u'always', u'fresh', u'fish', u'and', u'summarization', u'on', u'135', u'is', u'nice', u'place', u'in', u'the', u'end', u'of', u'the', u'night', u'market']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = word_list\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'good',\n",
       " u'food',\n",
       " u'and',\n",
       " u'friendly',\n",
       " u'service',\n",
       " u'the',\n",
       " u'restaurant',\n",
       " u'is',\n",
       " u'on',\n",
       " u'upper',\n",
       " u'night',\n",
       " u'market',\n",
       " u'close',\n",
       " u'to',\n",
       " u'the',\n",
       " u'railway',\n",
       " u'road',\n",
       " u'fish',\n",
       " u'and',\n",
       " u'seafood',\n",
       " u'is',\n",
       " u'very',\n",
       " u'popular',\n",
       " u'here',\n",
       " u'not',\n",
       " u'expensive',\n",
       " u'and',\n",
       " u'always',\n",
       " u'fresh',\n",
       " u'fish',\n",
       " u'and',\n",
       " u'summarization',\n",
       " u'on',\n",
       " u'135',\n",
       " u'is',\n",
       " u'nice',\n",
       " u'place',\n",
       " u'in',\n",
       " u'the',\n",
       " u'end',\n",
       " u'of',\n",
       " u'the',\n",
       " u'night',\n",
       " u'market']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 44)\n"
     ]
    }
   ],
   "source": [
    "tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n",
    "print(len(tokens_ids[0]),len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 44)\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_ids[0]),len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "input_ids = pad_sequences(tokens_ids,\n",
    "                          maxlen=MAX_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")\n",
    "#input_ids = pad_sequences([convert_word2idx(txt) for txt in tokenized_texts],\n",
    "                          #maxlen=MAX_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(MAX_LEN)\n",
    "for i in tokens_ids:\n",
    "    if len(i) > MAX_LEN:\n",
    "         \n",
    "        i = i[:128]\n",
    "        print(\"need more\")\n",
    "         \n",
    "        #MAX_LEN = len(i)\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n",
    "len(t_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12548  6996   781  8604   473  5816 12951 10613 16929  6201  5885  6298\n",
      "  8082  2462  5816 16629 12043 16992   781  1158 10613  7407  8352   121\n",
      "  6496 12988   781  2815  3648 16992   781  2728 16929 18612 10613 18270\n",
      "   263 10624  5816 14841 16936  5816  5885  6298     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"int64\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train test\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to torhc tensor\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16815, 128])\n",
      "torch.Size([16815, 128])\n",
      "torch.Size([16815, 128])\n"
     ]
    }
   ],
   "source": [
    "print(tr_inputs.shape)\n",
    "print(tr_masks.shape)\n",
    "print(tr_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1869, 128])\n",
      "torch.Size([1869, 128])\n",
      "torch.Size([1869, 128])\n"
     ]
    }
   ],
   "source": [
    "print(val_inputs.shape)\n",
    "print(val_masks.shape)\n",
    "print(val_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-30 17:16:48,694 : INFO : loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz from cache at /home/chantana/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
      "2019-10-30 17:16:48,696 : INFO : extracting archive file /home/chantana/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05 to temp dir /tmp/tmpZsjKam\n",
      "2019-10-30 17:16:59,157 : INFO : Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1, \n",
      "  \"hidden_act\": \"gelu\", \n",
      "  \"hidden_dropout_prob\": 0.1, \n",
      "  \"hidden_size\": 1024, \n",
      "  \"initializer_range\": 0.02, \n",
      "  \"intermediate_size\": 4096, \n",
      "  \"max_position_embeddings\": 512, \n",
      "  \"num_attention_heads\": 16, \n",
      "  \"num_hidden_layers\": 24, \n",
      "  \"type_vocab_size\": 2, \n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2019-10-30 17:17:09,836 : INFO : Weights of BertForTokenClassification not initialized from pretrained model: [u'classifier.bias', u'classifier.weight']\n",
      "2019-10-30 17:17:09,839 : INFO : Weights from pretrained model not used in BertForTokenClassification: [u'cls.predictions.bias', u'cls.predictions.transform.dense.weight', u'cls.predictions.transform.dense.bias', u'cls.predictions.decoder.weight', u'cls.seq_relationship.weight', u'cls.seq_relationship.bias', u'cls.predictions.transform.LayerNorm.weight', u'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(u\"bert-large-uncased\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune BERT\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [15:35<2:20:22, 935.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.260970212766\n",
      "Validation loss: 0.0463451605278\n",
      "Validation Accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 2/10 [31:25<2:05:20, 940.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.39494759739\n",
      "Validation loss: 0.0208820802086\n",
      "Validation Accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 3/10 [47:13<1:49:56, 942.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.432433886632\n",
      "Validation loss: 0.0181698585889\n",
      "Validation Accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 4/10 [1:03:03<1:34:27, 944.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.421093954462\n",
      "Validation loss: 0.0131548909162\n",
      "Validation Accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 5/10 [1:18:49<1:18:45, 945.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.581480946882\n",
      "Validation loss: 0.014533672983\n",
      "Validation Accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 6/10 [1:34:34<1:03:00, 945.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.856716735911\n",
      "Validation loss: 0.0151666778136\n",
      "Validation Accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 7/10 [1:50:21<47:16, 945.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.846808958049\n",
      "Validation loss: 0.0129837159639\n",
      "Validation Accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 8/10 [2:06:05<31:30, 945.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.699468779936\n",
      "Validation loss: 0.0164961499699\n",
      "Validation Accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 9/10 [2:21:51<15:45, 945.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.443237907206\n",
      "Validation loss: 0.0119038588434\n",
      "Validation Accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 100%|██████████| 10/10 [2:37:36<00:00, 945.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.751363191499\n",
      "Validation loss: 0.0165104018734\n",
      "Validation Accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    #print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels, true_inputs = [], [],[]\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        inputs = b_input_ids.to('cpu').numpy()\n",
    "         \n",
    "        true_inputs.append(inputs)\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n",
    "\n",
    "\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "all_data = 0\n",
    "preds = []\n",
    "vals = []\n",
    "for i,j,ll in zip(pred_tags,valid_tags,val_inputs):\n",
    "    for k,l,kk in zip(i,j,ll):\n",
    "        preds.append(k)\n",
    "        vals.append(l)\n",
    "        if k==l   : \n",
    "            count += 1\n",
    "            #print(k,l,idx2word[kk.item()])\n",
    "        all_data += 1\n",
    "print(count)\n",
    "print(all_data)\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = [u'B-RORG', u'I-RORG',u'B-RLOC',u'I-RLOC','0','B-FOOD','I-FOOD']\n",
    "print(classification_report(preds,vals, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
